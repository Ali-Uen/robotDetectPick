# robotDetectPick â€“ Objekterkennungsbasiertes Greifsystem (Bachelorarbeit)

Dieses Repository enthÃ¤lt die begleitende Implementierung zur Bachelorarbeit **â€Entwicklung und Implementierung eines objekterkennungsbasierten Greifsystems fÃ¼r einen Roboterarmâ€œ**. Ziel ist es, Objekte auf einem laufenden FÃ¶rderband mit YOLOv5 zu erkennen, ihre Greifpositionen mithilfe von KameraÂ­kalibrierung & SolvePnP in Welt- und Roboterkoordinaten zu bestimmen und sie anschlieÃŸend mit einem **Niryo Ned2** zu greifen.

---

## âœ¨ Kernfunktionen
- **Objektdetektion** mit YOLOv5 (Webcam/Stream)
- **Kamerakalibrierung** (Schachbrett) & **Pose-SchÃ¤tzung** via `solvePnP`
- **Weltkoordinatensystem** (Landmarken an den FÃ¶rderbandâ€‘Ecken) & KoordinatenÂ­transformation in das RoboterÂ­koordinatensystem
- **Pick&Place** auf einem **Niryo Ned2** inkl. Greifersteuerung & FÃ¶rderband
- **Echtzeitâ€‘GeschwindigkeitsabschÃ¤tzung** von Objekten fÃ¼r dynamisches Greifen
- **Evaluation** (Excelâ€‘Export) von Zeiten, Erfolgsraten und StabilitÃ¤t

---

## ğŸ“ Projektstruktur

```
robotDetectPick/
â”œâ”€ parallelProcess.py            # Startet Detektion, SolvePnP und Roboter als Threads
â”œâ”€ frame1.jpg                    # Beispielbild
â”œâ”€ yolov5/                       # Upstream YOLOv5 (detect.py, train.py, val.py, ... + *.pt)
â”œâ”€ solvepnp/
â”‚  â”œâ”€ camera.py                  # Kamera-Kalibrierung (Schachbrett)
â”‚  â””â”€ SolvePNP.py                # Pixelâ†’Welt: SolvePnP + Geschwindigkeit
â”œâ”€ robot/
â”‚  â”œâ”€ robotAndDetect.py          # Hauptlogik Robotersteuerung + Detektions-Integration
â”‚  â”œâ”€ pickAndPlace.py            # High-Level BewegungsablÃ¤ufe
â”‚  â”œâ”€ workspace.py               # Workspace (4 Ecken) am Ned2 anlernen
â”‚  â”œâ”€ coordinates.py, converter.py, color.py, sound.py
â”‚  â”œâ”€ robotcoordinates/*.txt     # Referenzpunkte Roboter-KS
â”‚  â””â”€ sounds/*.mp3               # Sprachhinweise (gTTS)
â”œâ”€ evaluation/
â”‚  â”œâ”€ evaluation.py              # Excel-Schreiber (Ergebnisse)
â”‚  â””â”€ ergebnisse.xlsx            # Ergebnisdatei (wird angelegt/fortgeschrieben)
â””â”€ weights/                      # Platz fÃ¼r eigene/feinjustierte Gewichte (optional)
```

> Hinweis: `parallelProcess.py` enthÃ¤lt teils absolute Pfade aus dem Entwicklungssetup. Bitte im Abschnitt **Konfiguration** anpassen.

---

## ğŸ§° Voraussetzungen

**Hardware**
- Niryo **Ned2** mit Greifer (oder Vakuumpumpe) und **Niryo Conveyor Belt**
- 2Dâ€‘Webcam (z.â€¯B. Logitech) auf das FÃ¶rderband ausgerichtet
- Rechner mit Python (getestet mit 3.8â€“3.11) und ggf. NVIDIAâ€‘GPU (optional fÃ¼r schnellere YOLOâ€‘Inference)
- Netzwerkverbindung zum Ned2 (LAN/WLAN)

**Software / Pythonâ€‘Pakete**
- PyTorch + AbhÃ¤ngigkeiten aus `yolov5/requirements.txt`
- ZusÃ¤tzlich: `pyniryo`, `gTTS`, `openpyxl`, `pandas`

Beispiel (innerhalb eines frischen venv):
```bash
pip install -r yolov5/requirements.txt
pip install pyniryo gTTS openpyxl pandas
```

---

## âš™ï¸ Konfiguration

1) **Repositoryâ€‘Pfad**
   - Empfohlen: in das Homeâ€‘Verzeichnis klonen/entpacken als `~/robotDetectPick`.
   - In `parallelProcess.py` ggf. absolute `sys.path.append(...)`-EintrÃ¤ge anpassen oder durch relative Pfade ersetzen.

2) **Robotâ€‘IP & Workspaceâ€‘Name**
   - In `robot/robotAndDetect.py` und `robot/stopConveyor.py` die Variable `robot_ip_address` auf die IP deines Ned2 setzen.
   - Den gewÃ¼nschten `workspace_name` eintragen (siehe Workspaceâ€‘Anlernen).

3) **YOLOâ€‘Gewichte**
   - Standardgewichte (`yolov5s.pt`, `yolov5m.pt`) liegen unter `yolov5/`.
   - Eigene/feinjustierte Gewichte (z.â€¯B. `best.pt`) ablegen (z.â€¯B. in `weights/`) und den Pfad in den Skripten/Parametern angeben.

4) **Kameraâ€‘Index / Quelle**
   - In der Regel Webcam = `--source 0` (siehe Startvarianten unten).

---

## ğŸ¯ Weltkoordinatensystem & Landmarken

- Bringe an den **vier Ecken** des FÃ¶rderbands Landmarken (markante Marker) an.
- Definiere das Weltkoordinatensystem mit Ursprung **oben links**; **yâ€‘Achse ~30â€¯cm**, **xâ€‘Achse ~14,5â€¯cm**.
- `SolvePNP.py` erwartet **4 korrespondierende 3Dâ€‘Weltpunkte** und **4 2Dâ€‘Pixelpunkte** (Landmarken) aus der Detektion.
- Die Transformation Pixelâ†’Weltâ†’Roboter erfolgt anschlieÃŸend automatisch in der Pipeline.

> Tipp: Die Kameraperspektive sollte mÃ¶glichst orthogonal/ohne starke SchrÃ¤glage montiert sein. Nutze die Schachbrettâ€‘Kalibrierung (s. unten) vorab.

---

## ğŸ¥ KameraÂ­kalibrierung (einmalig pro Aufbau)

1) Drucke ein **Schachbrettmuster** und nimm mehrere Bilder aus unterschiedlichen Winkeln auf.
2) Lege die Bilder in den von `solvepnp/camera.py` erwarteten Ordner und fÃ¼hre aus:
   ```bash
   python solvepnp/camera.py
   ```
3) Die Skripte berechnen **Kameramatrix** und **Verzerrungen**; diese werden in der SolvePnPâ€‘SchÃ¤tzung verwendet.

---

## ğŸš€ Startvarianten

### Variante A: Alles aus einem Skript (Threadâ€‘basiert)
```bash
python parallelProcess.py
```
Startet drei Threads:
- **Detektion** (YOLOv5 Webcamâ€‘Stream)
- **SolvePnP + Geschwindigkeit** (Pixelâ†’Welt, Tracking)
- **Robotersteuerung** (Ned2: Greifen/Platzieren, Conveyor Control)

> PrÃ¼fe vorab die Pfade in `parallelProcess.py` und die `robot_ip_address`.

### Variante B: Manuell in separaten Terminals
1) **Detektion** (YOLOv5 Webcam):
   ```bash
   cd yolov5
   python detect.py --weights yolov5m.pt --source 0 --conf-thres 0.5
   ```
2) **SolvePnP & Geschwindigkeit**:
   ```bash
   python solvepnp/SolvePNP.py
   ```
3) **Robotersteuerung**:
   ```bash
   python robot/robotAndDetect.py
   ```

---

## ğŸ“ Training (optional)

- Datensatz (YOLOâ€‘Format) z.â€¯B. via Roboflow exportieren.
- Training mit YOLOv5:
  ```bash
  cd yolov5
  python train.py --img 640 --batch 16 --epochs 150 --data data.yaml --weights yolov5m.pt
  ```
- Die resultierenden Gewichte `best.pt` anschlieÃŸend in der Detektion verwenden.

---

## ğŸ“Š Evaluation

- WÃ¤hrend der Laufzeit werden Messwerte (Zeiten, Koordinaten, Erfolgsraten/StabilitÃ¤t) nach `evaluation/ergebnisse.xlsx` geschrieben.
- Das Format wird in `evaluation/evaluation.py` definiert. Zum reinen Stoppen des FÃ¶rderbands existiert `robot/stopConveyor.py`.

---

## ğŸ›¡ï¸ Sicherheit & Hinweise

- Notâ€‘Aus am Ned2 bekannt machen; GreiferhÃ¶he richtig konfigurieren (Kollisionsgefahr!).
- FÃ¶rderband zunÃ¤chst langsam testen und Greifpunkte validieren.
- Kamera und Landmarken fixieren, sonst driften Koordinaten.
- Rote Objekte (falls so gelabelt) ggf. in der Logik ausschlieÃŸen/ignorieren.

---

## â—ï¸Typische Stolpersteine

- **Keine Verbindung**: PrÃ¼fe `robot_ip_address`, Netzwerk und Kalibrierungsstatus des Ned2.
- **Detektion findet nichts**: `--source`/Kameraâ€‘Index prÃ¼fen, Gewichtsdatei korrekt?
- **Koordinaten falsch**: Kamerakalibrierung wiederholen, Landmarkenâ€‘Reihenfolge in SolvePnP beachten, Workspace am Roboter neu anlernen.
- **Pfadprobleme**: `parallelProcess.py` auf relative Pfade umstellen oder Repo in `~/robotDetectPick` ablegen.

---

## ğŸ“„ Lizenz & Danksagung

- Teile dieses Repos verwenden **YOLOv5** (Ultralytics) â€“ **GPLâ€‘3.0**. Beachte die Lizenzdateien im `yolov5/`â€‘Ordner.
- Hardware/SDK: **Niryo Ned2**, **PyNiryo**, **Niryo Conveyor**.
- Dieses Projekt entstand im Rahmen einer Bachelorarbeit an der TH KÃ¶ln.

---

## ğŸ‘¤ Autor & Kontakt

- Autor: Ali Ãœnal
- Projekt: Bachelorarbeit â€“ Objekterkennungsbasiertes Greifsystem (TH KÃ¶ln)

